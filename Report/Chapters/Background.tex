\chapter{Background}
\label{cha:background}

In order to understand how and why heterogeneous systems can be applied to the problem of
bitcoin mining, it is first important to obtain an understanding of how the bitcoin currency
works and what heterogeneous computing is.

\section{The Bitcoin Currency}
\label{sec:bitcoins}
Bitcoin, often abbreviated BTC or \bitcoin, is a digital currency, using a peer-to-peer network to provide
a decentralized currency, not relying on banks or financial institutions to process transactions
or maintain accounts. An account simply consists of a cryptographic keypair, which is used to sign
and verify transactions, and as such anyone can create as many accounts as they wish. The bitcoin
project provides a variety of software for interacting with the bitcoin network and managing
accounts.

At the core of the bitcoin system is the block chain, a distributed, linked list consisting of blocks
which contains the transactions that have been executed on the network since the previous block was
generated.

A block is only valid if the arithmetic value of the double SHA-256 hash of its header is below
a certain target value. Finding a block that has such a header is computing intensive because it
has to be done through a brute-force search, and this prevents the network from being flooded with
new blocks. The target value is decided by the network ``difficulty'' and is set to such a value
that, on average, six new blocks are generated per hour.

% TODO: Add figure from Bitcoin paper

To get people to participate in creating new blocks, a reward is offered to whomever manages to
create a block. This reward also ensures that more money is added into circulation. The size of
the reward decreases over time until a predetermined number of bitcoins have entered circulation.
The reward is currently 25 bitcoins and halves every 210~000 blocks. The total number of bitcoins
that is to be generated is 21~000~000 bitcoins. \cite{bitcoin}

\subsection{Mining Bitcoins}
\label{sec:bitcoin-mining}

The process of creating a new block for the bitcoin blockchain is often referred to as \textit{mining},
refering to the fact that a lot of energy may need to be expended in order to find a valid block.

The process begins with the creation of a transaction that transfers the reward for generating the block
into the account of the miner. This transaction is often called the coinbase or generation transaction.
All transactions transmitted to the bitcoin network since the last block was generated are gathered and
the hash of each transaction is inserted into a merkle tree, which is a tree where where every interior
node is labelled with the hash value of its children.

The root of the merkle tree is inserted into the header for the new block together with the hash of the
previous block and various other fields specified by the standard. If the hash of the header is below the
target value, the block is successfully mined and transmitted to the network. If the hash does not satisfy
the demands of the network, a 32-bit field in the block header, called the \textit{nonce}, can be changed
to produce a new hash. In addition, transactions can be excluded in order to produce a different merkle
root for the header, effectively providing a much larger search space.

If more than one new block is pushed to the network at the same time, the block chain diverges.
This situtation is resolved when the next block is generated; the longest block chain is then accepted
as the canonical block chain by the rest of the network. \cite{bitcoin}

\subsection{Pooled Bitcoin Mining}

Because an increasing amount of computing power is being used to mine bitcoins, the difficulty of finding
a block has increased to such a level that no single bitcoin miner can hope to find a block on his or her
own any longer. Figure \ref{fig:difficulty} illustrates how the difficulty level have risen on the bitcoin network
since the beginning. The hashrate of systems used in bitcoin mining is measured in double SHA-256 hashes per second,
abbreviated H/s. The current total hashrate is about 396,5~EH/s\footnote{$396,5\cdot 10^{18}$ H/s} according to the
statistics available from \texttt{blockchain.info}. This means that the amount of work needed to find a single block is,
on average, 237,9~ZH\footnote{$237,9\cdot 10^{21}$ hashes}. Using, for instance, a GPU-based bitcoin miner with a
hashrate of 1~GH/s, it would take, on average, $237,9\cdot 10^{14}$ seconds, or 7~543~759 years to find a block.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.95\textwidth]{Figures/Bitcoin/Difficulty-all}
    \caption{The bitcoin mining difficulty, from the beginning until today \cite{bitcoin-charts}.}
    \label{fig:difficulty}
\end{figure}

To overcome this problem, mining pools were invented. A bitcoin mining pool is a service that distributes
work between miners. The work that is distributed has a lower target value than does the bitcoin network
itself, meaning that each participating miner can find valid blocks faster. Although most of the blocks
submitted to a pool does not fulfill the network requirements, sometimes a block fulfills both the
pool's difficulty requirements \emph{as well as} the network requirements; in that case the new block is
submitted to the bitcoin network and all the miners who submitted work towards finding the block
is rewarded according to their contribution.

As such, pooled mining lets anyone with weaker hardware participate in bitcoin mining in exchange for
a smaller reward. However, using hardware such as regular CPUs and GPUs is still considered unprofitable
because of these processors' relative performance compared to specialized bitcoin mining ASICs, as
discussed in section \ref{sec:bitcoin-history}.

\section{The SHA-256 Hash Algorithm}

The SHA-256 hash algorithm is defined in FIPS180-4 \cite{fips180-4} and is a member of the SHA-2
family of hash functions. Like all hash functions, it works by accepting an arbitrary
amount of input data and producing a constant-length output. The function is one-way,
that is, given a hash, it is impossible to determine the input data used to produce
the hash.\todo{Mention all 3 attributes}

The SHA-256 algorithm works on blocks of 512~bits. Each of these blocks are expanded into a
block of 64 32-bit long words using a message expansion function. Each of these words are
used as input to one iteration of the SHA-256 compression algorithm, which uses simple bitwise functions
such as shifts, rotations, logic functions and unsigned addition to create a 256~bit
output value. Each round of the compression function is dependent on the output data from
the previous round in addition to the current input word from the expanded message block
and a constant.

After 64 iterations of the compression function, the result is added to the intermediate
hash values from any previous blocks, or, if there are no previous blocks, the initial
hash value, producing a finished hash.

A more detailed description of the algorithm can be found in appendix \ref{app:hashing-algo}.

In the bitcoin system, a double SHA-256 algorithm is used. To obtain a double hash, the
output of a previous SHA-256 calculation is simply used as input data to the hashing
algorithm. \cite{fips180-4}

% Is this too little or too vague?

\section{Evolution of Bitcoin Mining Hardware}
\label{sec:bitcoin-history}

Because of the bitcoin currency's popularity and the competitiveness involved in mining it, there exists
many different accelerators to improve the performance and energy efficiency of mining systems.
During the currency's history, hardware for bitcoin mining has evolved from regular, general-purpose
CPUs to highly specialized ASIC-based systems.

The bitcoin blockchain was started on January 3rd, 2009. At this point, all mining was done
using CPU mining. The official bitcoin network client supports mining and was used for this
purpose. The fastest CPU miner, a high-end, overclocked Core i7 990x eventually reached 33~MH/s
using SIMD extensions in order to improve performance. \cite{bitcoin-history}

\subsection{GPU Mining}
The shift to GPU mining started in July 2010, when the first OpenCL miner was written and
used in a private mining setup. In September that year, the first open source GPU miner,
which was based on Cuda, was released after the author was paid 10~000 bitcoins, worth about
\$600 USD at the time, by Jeff Garzik, one of the core bitcoin developers. An open source
OpenCL-based miner was released shortly after \cite{bitcoin-history}.

OpenCL miners typically compute the double SHA-256 hash using a fully unrolled implementation
of the compression loop. Multiple hash calculations are run simultaneously exploiting the parallelism
offered by GPUs. Many miners tweaked various parameters of their hardware, such as the voltage and
clock frequencies of both the video RAM and the GPU core in order to get higher throughput
and thus reduce the cost of running each GPU, in order to obtain greater profits \cite{bespoke-silicon}.
% Maybe mention rigs?

\subsection{FPGA Mining}
\label{sec:fpga-mining}

FPGA miners appeared in June 2011, providing better energy efficiency compared to GPUs \cite{bespoke-silicon}.
Early designs were mostly based on the Spartan 6 FPGAs from Xilinx, and could provide a
performance of between 200~MH/s and 220~MH/s per chip, about the same as contemporary GPUs \cite{bitcoin-hardware-cmp},
but consuming as little as a fifth of the power used by their GPU counterparts \cite{bespoke-silicon}.

% Note: Taylor managed to get the release year of the GPUs he compares performance to wrong,
% choosing GPUs from 2012 to compare to FPGAs from 2011.

FPGAs provided great advantages to the speed of bitwise functions, which are important for
the SHA-256 algorithm. Popular open-source designs were created so that they could be used
by different kinds of FPGAs. They consisted of a SHA-256 module which had a configurable depth pipeline, each
consisting of the necessary hardware for computing a specific number of rounds of the
SHA-256 compression function. Completely unrolling the algorithm created a pipeline of
64 stages, with a throughput of 1 hash per cycle, but it was also possible to specify
lesser unroll factors which resulted in fewer pipeline stages and reduced throughput, taking
several cycles to perform a hash, but saving registers and logic resources in the FPGAs.
The pipelines were duplicated to improve performance.

Power consumption became much higher than typical for FPGAs, with the pipelined design
giving an extremely high activity factor for LUTs. Development boards could not provice enough
power and heat dissipation for sustained usage, and development of custom boards that focused
on providing enough power and cooling with minimal unused resources, became popular. \cite{bespoke-silicon}

\subsection{ASIC Miners}
\label{sec:asic}

Neither GPUs nor FPGAs can compare to ASICs, specialized chips that started to appear on the network
in the beginning of 2013 \cite{first-asic-miner}. It was the need for more high-performance and power
efficient bitcoin miners caused implementors of bitcoin miners to turn to ASIC solutions.

One of the first ASIC designs were Butterfly Labs' ASIC miners. Having experience developing
FPGA based mining solutions, the company created an ASIC chip with 16 double SHA256 modules,
the equivalent of 16 FPGA-based miners using a 65~nm process. The end-product ended up being
delayed when the chips ended up consuming more power than expected and had to be underclocked
in order to be able to cool the chips properly.

ASICMINER was another of the earliest designs, including only one SHA256 hash unit on a chip,
replicating a single FPGA-based design at a much higher frequency, with better energy efficiency
and a cheaper price. \cite{bespoke-silicon}

\subsubsection{The Goldstrike 1}
Another ASIC design is the Goldstrike 1 architecture, noteworthy because its architecture is
described in an article in IEEE Micro by Javed Barkatullah and Timo Hanke \cite{goldstrike}.
Because of the competitive nature of bitcoin mining, it is not common for designers of ASIC
solutions to reveal too many details about the architecture of their chips and few scientific
papers exist on the topic.

The Goldstrike is an ASIC-based bitcoin mining solution capable of reaching 2~TH/s. To reach the performance
requirements, a bitcoin mining core was created that was able to perform at 125 GH/s. Four of
these were combined in one package, with four of these packages then combined on a circuit board
to produce the target performance.

The Goldstrike cores consist of an architecture with 120 hash engines running in parallel.
A hash engine searches through all possible values of the 32-bit nonce field in the bitcoin header
(see section \ref{sec:bitcoin-mining}) in order to find a valid hash.

The hash engines uses a pipelined double-SHA-256 implementation, with each round of the hash calculation
unrolled into a pipeline.

The finished ASIC chips, each with four bitcoin mining cores, were measured to provide 504~GH/s while using
500~W of power. This gives an energy efficiency of about 1~GH/J.

Interestingly, the architecture provided challenges with regards to to powering and cooling the chips.
Because the architecture uses all available computing power for bitcoin mining all the time while
the chips are in use, no parts of the chips can be turned off to save power. \cite{goldstrike}

% Fun fact, GH/Ws = GH/J

\subsection{Scaling of bitcoin hardware}
Due to the dark silicon problem, future chips will require that parts be powered down or deactivated
in order to satisfy a given power budget \cite{dark-silicon2}. Bitcoin mining may be close to
the worst case for dark silicon optimizations, far beyond multicore CPUs or GPUs. Due to bitcoin
mining's high processing requirements, the logic needed to run the algorithm cannot be turned off without losing
performance.

Butterfly Labs ran into this problem with their 65~nm chip, and had to scale back the performance to reduce power consumtion
and manage to cool the chip properly. While bitcoin mining began as a ``race to ASIC'', energy costs now determine
which ASICs are the most profitable and therefore energy efficiency is a problem that must be addressed. \cite{bespoke-silicon}

%Figure \ref{fig:bitcoin-difficulty-devices}, as seen in \cite{bespoke-silicon}, shows the same up to 2014, but also hightligts when each generation was released, from CPU to ASIC.
%
%\textbf{Comment the development. Find facts about it. (currently lower priority)}

%\begin{figure}[htb]
%    \centering
%    \includegraphics[width=0.95\textwidth]{Figures/Bitcoin/Difficulty-devices}
%    \caption{The BTC Mining Difficulty, compared to when each generation was introduced, as seen in \cite{bespoke-silicon}.}
%    \label{fig:bitcoin-difficulty-devices}
%\end{figure}

\section{The Problem of Dark Silicon}
\label{sec:dark-silicon}

Moore's law states that transistor density on integrated chips will continue to double every
two years. In addition, native transistor speeds are increasing with a factor of 1,4.
According to Dennard scaling, which predicted that the power density of transistors, that
is the voltage and current used to operate the transistor, would scale down with the
size of the transistors, this would not be a problem; however, the energy efficiency
of transistors is only improving by a factor of 1.4, not keeping up with the scaling
of transistor size.

Under a constant power-budget, there is a shortfall of a factor of 2 in the energy budget,
and this utilization potential of a chip is falling exponentially by 2 times for each generation.
If the power limitation were to be based on the current generation, then designs would be 93,75\% dark in eight years.
This gives rise to the term ``dark silicon'', where a chip must either be underclocked or parts of
it turned off in order to stay within a defined power budget, giving rise to ``dark'' areas of the chip.
This is especially true for chips where the cooling solutions are no longer efficient enough to remove
the generated heat from a fully powered chip.

In an attempt to work around this problem, the CPU industry moved to using multicore processors around 2005.
Since the gains from improving ILP was diminishing, the industry focues more on TLP and throughput through multiprocessing, the only other known way of scaling at the time. \cite{computer_architecture} 
However, adding multiple cores does not circumvent the problem in the long run.
Multicore chips will not scale as transistors shrink, and the fraction of a chip that can be filled with cores running at full frequency is dropping exponentially with each processor generation. 
Large fractions of the chip will be left dark --- either switched off for a long time, or significantly underclocked.
New designs are required, with new architectural techniques to spend area to buy energy efficiency. \cite{dark-silicon}

\subsection{Approaches to the Problem of Dark Silicon}
\label{sec:taylor}

To work around the problem of dark silicon, several approaches have been suggested.
In \cite{dark-silicon}, Taylor have listed up three particular approaches: shrinking the chips, dimming the chips, and specializing the chips.
In addition, there is always the possibility that future technology will be a ``deus ex machina'' and solve the problem in
an unexpected fashion.

\subsubsection{Shrinking the Chips}

Instead of having dark silicon on the chip, one can simply shrink the chip itself.
%Taylor\cite{dark-silicon} views these "shrinking chips" as the most pessimistic outcome.
%Although all chips may eventually shrink somewhat, the ones that shrink the most will be those where dark silicon cannot be applied fruitfully to improve the product.
%These chips may rapidly turn into low-margin businesses for which further generations of Moore’s law provide small benefit.
% %*** NOTE: The above makes no sense.

% %*** Torbjørn, NOTE RESPONSE: Very well, here is my second attempt:

All chips may be shrunk to a small extent, but the only chips that truly can gain from only shrinking, will be those on which applied dark silicon is only a waste, and cannot further enhance the product.   
Specalizing into these types of shrunk chips may turn out to be less profitable business, as the further generations of Moore's law adds little benefit. 
% Torbjørn: Better now?

Futhermore, there are other disadvantages: Exponentially smaller chips are not exponentially cheaper, since mask cost, design cost and I/O pad aread cannot be amortized. 
Competition will most likely favor chips that utulizes dark silicon to improve the overall product, thus chips that are only shrinked will sell at low market price, leading to loss for the company.
Exponential shrinking also leads to exponential rise in power density, and chip temperature will thus follow suit.
Meeting the temperature limit will increase the cost, at which the scaling of energy efficiency is reduced below the nominal 1.4 factor.
%Meeting the temperature limit will reduce the scaling below the expected energy effeciency, at the nominal \todo{Feels weird}1.4X scaling.

%There are also second-order effects assosiated with shrinking chips:

%\begin{itemize}
%    \item Exponentially smaller chips are not exponentially cheaper.
%    In addition to the silicon itself, cost include mask costs, design costs and I/O pad area.
%    These cannot be amortized, and the price will increase per mm$^2$, as the chip shrinks.
%    \item Shrinking the silicon can also shrink the chip selling price, but competition will likely force companies to utilize dark silicon if it can attain a benefit for the end product.
%    Companies who would rely on shrinking only, may loose the competition and sell chips at catastrophically decreased chip prices.
%    \item Exponential shrinking leads to exponential rise in power density, and chip temperature will thus follow suit.
%    Meeting the temperature limit will reduce the scaling below the nominal 1,4x expected energy efficienty. 
%\end{itemize}

%\subsubsection{The Dim Horseman}
\subsubsection{Dimming the chip}
While the fractions of dark transistors of the chip increases exponentially, the silicon area becomes exponentially cheaper
as a resource, relative to power and energy consumption. This gives the opportunity to spend area to buy energy efficiency,
and basing new architectures on this. \todo{Consider adding a figure to better illustrate the point}
Dark silicon areas can be populated with logic that is used only part of the time.

The term ``dim silicon'' refers to the use of general purpose logic that typically employs heavy underclocking or infrequent use.
As such, large amounts of otherwise dark silicon area is put to productive use while meeting the power budget.

%The early 90-nm designs of, for instance, Cell and Prescott, were dimmed because actual power exceeded design-estimated power.  % Citation needed!
% Torbjørn: Nah... not so important.

Lately, more elegant methods are emerging. Among the dim silicon techniques are dynamically varying the frequency with the
number of cores being used, scaling up the amount of cache logic, employing near threshold voltage (NTV) designs, and redesigning
architectures to accommodate bursts that temporarily allow the power budget to be exceeded, such as Intel's Turbo Boost technology. \cite{dark-silicon}

% %\todo{This subchapter ended up long. Try to see if it can be reduced. Especially NTV}
%According to Taylor\cite{dark-silicon}, as exponentially larger fractions of a chip’s transistors become dark transistors, silicon area becomes an exponentially cheaper resource relative to power and energy consumption.
%Therefore new architectural techniques that spend area to buy energy efficiency is called for.
%Instead of shrinking silicon, one may consider populating dark silicon area with logic that is used only part of the time, and interesting new design possibilities occurs.%The term "dim silicon" refers to techniques that put large amounts of otherwise-dark silicon area to productive use by employing heavy underclocking or infrequent use to meet the power budget.
%The architecture has to strategically managing the chip-wide transistor duty cycle to enforce the overall power constraint. 
%Whereas early 90-nm designs such as Cell and Prescott were dimmed because actual power exceeded design-estimated power, more increasingly more elegant methods are converging, that make better trade-offs.%Among the dim silicon techniques are dynamically varying the frequency with the number of cores being used, scaling up the amount of cache logic, employing near threshold voltage (NTV) processor designs, and redesigning the architecture to accommodate bursts that temporarily allow the power budget to be exceeded, such as Turbo Boost and computational sprinting.

% %\todo{Consider shorting down or removing entire lists from each sub-chapter, to keep the four horsemen short and concise.}Amond dim silicon methods are: 

%\begin{itemize}
%    \item Turbo boost, where less core are active, the higher the frequency they may run at.
%    Energy gained from turning off cores is used to increase voltage and frequency of active cores.
%    This is known as Dynamic voltage and frequency scaling (DVFS).
%    \item Near-threshold voltage (NTV) Processors.
%    Transistors were at 2013 operated around 2,5x the threshold voltage, an energy-delay optimal point. 
%    This is at a point where reducing the voltage severely affects the frequency drop, which reduces the effective gain from downward-DVS.
%    Nevertheless, researchers have begun exploring this regime.
%    NTV logiv is a recent approach, where transistors in the near-threshold regime are operated slightly above the threshold voltage.
%    This provides a more palatable trade-off between energy and delay than subthreshold circuits, for which frequency drops exponentially with voltage decreases.
%    \todo{Give a proper mention of where the following numbers were taken from}
%    Although NTV per-processor performance drops faster than the corresponding savings in energy-per-instruction (5X energy improvement for an 8x performance cost), the perfor- mance loss can be offset by using 8x more processors in parallel if the workload allows it.
%    Then, an additional 5x processors could turn the energy efficiency gains into additional performance. 
%    \todo{Have to admit, I still don't understand how this is a gain. And this is almost the point of too much. Should remove the example, and find a different way to explain}So, with ideal parallelization, NTV could offer 5x the throughput improvement by absorbing 40x the area. 
%    But this would also require 40x more free parallelism in the workload relative to the parallelism consumed by an equivalent energylimited super-threshold many-core processor.%    \item Bigger caches.
%    Because only a subset of cache transistors (such as a wordline) is accessed each cycle, cache memories have low duty cycles and thus are inherently dark. 
%    Adding cache is therefore one way to simultaneously increase performance and lower power density per square millimeter.
%    But the less memory-bound a running application is, the less the benefit.
%    \item Computational sprinting and turbo boost.
%    One temporarily exceeds the nominal thermal budget but relies on thermal capacitance to buffer against temperature increases, and then ramp back to a comparatively dark state.
%    These are used within "race to finish" computations.

%\end{itemize}

\subsubsection{Specialization}
Instead of using general purpose logic, this approach focuses on specialized use of logic for selected tasks.
Within a set of processors, each are specialized for a subset of tasks, increasing energy efficiency or performance compared to a general purpose processor, for those particular tasks.
An application is executed on the processor which is deemed the most efficient for the task.
Cores not in use are power- and clock gated and do not waste energy unnecessarily.

Specialized accelerators are heavily used today, especially in fields such as media and graphics and mobile phone processors.
These enable orders-of-magnitude improvements in energy efficiency and performance for certain types of computations.
Systems with more coprocessors than general purpose processors, referred to as coprocessor-dominated architectures, or CoDAs, is predicted to become more and more common.

Challenges with these systems are expected as well, one of them being the so-called ``Tower of Babel'' crisis, where
the notion of general-purpose computation becomes fragmented, and the clear lines of communication between programmers and software and the underlying hardware is no more.
For instance, CUDA for NVidia GPUs cannot be used for similar architectures, such as AMD GPUs.

There are several cases of overspecialization problems between accelerators, where many of them cannot be used for closely related classes of computation.

%New heterogenenous hardware is more difficult to adopt for third party programmers, due to the cost of programming heterogeneous (where a clear example is the \todo{Hint: Maybe look up more examples?}Sony PlayStation 3, compared to Microsoft XBox). % Citation needed, also sentense makes no sense.
Specialized hardware also risk becomming obsolete when standards are revised. \cite{dark-silicon}

\todo{Just a reminding note: Look up the source code and argue among yourselves, regarding the PS3 vs XBOX example}

%\subsubsection{The Dues Ex Machina Horseman}
\subsubsection{Deus Ex Machina}
The termology ``deus ex machina'' comes from literature and theater, in which the protagonists seem increasingly doomed until the very last moment, when something completely unexpected comes out of nowhere to save the day.
In the case for dark silicon, a deus ex machina would be a breakthrough in semiconductor device technology.
The required breakthrough would have to be very fundamental, making it possible to build transistors out of devices other than MOSFETs. 
There are physical limits to what can be done with %\todo{I chose to not elaborate this one further. Remove when delivering, unless we change our mind}
leakage from MOFSET transistors, and transistors made of other materials may go beyond these limits.
New transistors must also be able to compete with MOFSETs in performance.
Tunnel field-effect transistors (TFET) and nanoelectromechanical system (NEMS) switches are examples on inventions that hint to order-in-magnitude improvements in the leakage problem, although they still fall short in performance. \cite{dark-silicon}

\section{Heterogeneous Architecture: Proven Examples}
\label{sec:heterogeneous}

Specialized heterogenenous architectures offers various possibilities for improved performance and energy efficiency;
this could be in the system's ability to adapt to various applications or even external conditions such as
power or temperature conditions \cite{heterogeneous-ee, heterogeneous-perf, heterogeneous-arch}.

\subsection{Energy Efficiency}
\label{subsec:rw_ee} 
Kumar \textit{et al} \cite{heterogeneous-ee} ran a simulation where they combined four generations of processors from the Alpha family: EV4 (Alpha 21064), EV5 (Alpha 21164), EV6 (Alpha 21264) and a single-threaded version of EV8 (Alpha 21464), and made a heterogenous multi-processor.
The architecture, with the chosen cores and their relative sizes to one another can be seen in figure \ref{fig:Kumar1}.
Only one application would run at the time, on one core, while the others were powered down.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/Heterogeneous/Kumar1}
    \caption{Cores used in \cite{heterogeneous-ee}, and their relative sizes.}
    \label{fig:Kumar1}
\end{figure}

14 benchmarks from SPEC2000 were used in this experiment, simulated using SMTSIM, and both energy and energy-delay were measured.

For the simulation, it was assumed that the system knew the needs of every application, and selected core statically. 
Results showed average energy savings of 32 \%, and average performance loss of only 2,6 \%, relative to using the EV8 core.
Kumar \textit{et al} also showed in this experiment that dynamic core switching outperforms the best static core selection.
Simple heuristics would sample the cores at regular intervals, and execute core switch based on the results.
The heuristics achieved up to 93 \% of the energy-delay gains compared to the best static selection. 

\subsection{Performance}
\label{subsec:rw_perf}
In another experiment, Kumar \textit{et al} proved that heterogenity can be exploited to gain increased performance, for multi-threaded workload.
They point out that heterogeneous architectures are advantagous for two reasons.

First, they offer efficient adaptation to application diversity, as applications differ in their resource needs.
Some are compute intensive and make good use of an out-of-order pipeline with high issue-width, while others may be memory-bound, and will underutilize advanced cores.
They may perform almost as well as an in-order core with low issue-width \cite{heterogeneous-perf}.

Second, heterogeneous architectures offer more efficient use of the die area for a given thread-level parallelism.
A complex core can be replaced by multiple smaller and simpler cores. 
Since the process or thread-level parallelism varies within most systems, a mix of cores that offer some large cores for high single-thread performce, and some small cores with high throughput per die area, is a potentially attractive area of research. \cite{heterogeneous-perf}

In contrast to the previous work described in \ref{subsec:rw_ee} by Kumar \textit{et al}, this experiment assigned multiple threads to multiple cores, and best global assignment was considered above best assignment for a selected application.
EV5 and EV6 from the Alpha family were chosen for this project, with one EV6 nearly equal to five EV5s in size.
With the total chip area anvailable for the architecture assumed to be 100 mm$^2$, the space could consist of maximum 4 EV6 cores, or 20 EV5 cores.
Three EV6 cores and five EV5 were chosen for this experiment, with the expectation that it would perform well over a wide range of available thread-level parallelism. 
A small number of SPEC2000 benchmarks were chosen, with the focus on evaluating varying numbers of threads.
SMTSIM and Simpoint were used for the simulation.

%\todo{Mind-blowing, consider removal}Evaluation metric was weighted speedup, which in this context is the arithmetic sum of the individual IPCs of the threads constituting a workload divided by their IPC on a baseline configuration when running alone.
%In addition to performance gain, application response time within varying queue lengths is tested as well, to give insight on how well heterogeneous processors handles large queues, compared to best homogeneous systems.

Best static scheduling ensured that threads that were least affected by the difference between the EV5 and the EV6, were assigned to the EV5 processors, when all EV6 processors were occupied.
When the number of threads passed 4, the measured weighted speedup increased on the heterogeneous system, compared to a homogeneous CMP with 4 EV6.
When the number of threads passed 13, a CMP with 20 EV5 performed better, though this can be changed with a different combination of EV5 and EV6 cores.
This is shown in figure \ref{fig:Kumar2}.

%\todo{Following 2 sentences are nearly ripoff from \cite{heterogeneous-perf}, consider making quote or what?}
Compared to 4 EV6 cores, the heterogeneous processor performed up to 37~\% better with an average 26~\% improvement over the configuration considering 1--20 threads. 
Compared to 20 EV5 cores, the performance was up to 2.3 times better, and averaged 23~\% better over that same range.
Using dynamic heuristics for core assignment further increased the performance. \cite{heterogeneous-perf}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/Heterogeneous/Kumar2}
    \caption{Weighted speedup, relative to EV6, when comparing the heterogenous multiprocessor with best homogeneous multiprocessors using EV5 and EV6 \cite{heterogeneous-perf}}
% %**NOTE: The figure text makes no sense!
% Better now?
    \label{fig:Kumar2}
\end{figure}

In addition to testing the performance gain, Kumar \textit{et al} also tested the the response time within the job queue as well.
This was to give insight on how well heterogeneous processors handles large job queues, compared to the best homogeneous systems.
For this test, jobs with an average distribution of 200 million cycles were generated randomly and executed.
Then different mean job arrival rates with exponential distribution were simulated.
It was revealed a great difference between saturation for a homogenous system with 4 EV6 processors and the heterogeneous processor.
For the former, the unbounded response time was seen as the arrival rate approached its maximum throughput around 2 jobs per 100 million cycles.
%From there, the run queue became infinite. % %**NOTE: This makes no sense!
Beyond this point, due to the saturation, the response time became infinate. %Better now?
The heterogeneous system remained stable well beyond that point.
The degradation was also more graceful under heavier loads than for homogeneous processors.
This can be seen in figure \ref{fig:Kumar3}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/Heterogeneous/Kumar3}
    \caption{Limiting response-time for various loads \cite{heterogeneous-perf}.}
    \label{fig:Kumar3}
\end{figure}

\subsection{Optimal architecture}
%\todo{Subchapter needs double check against source papers}
\label{subsec:rw_arch}
Kumar \textit{et al}\cite{heterogeneous-arch} has also taken a closer look at what can be considered good heterogeneous design.
They argue that while the previous experiments gave increased energy efficiency and performace, a heterogeneous system should be designed from scratch, instead of using pre-existing core designs.
While they surpassed homogeneous designs, pre-existing cores failed to reach the full potential of heterogenity for three reasons: they present low flexibility in choices, the core choices remain \todo{Citation needed for footnote}monotonic\footnote{\textit{Monotonicity} is defined as a property of a multicore architecture where there is a total ordering among the cores in terms of performance and this ordering remains the same for all applications. A multiprocessor consisting of Alpha EV5 and EV6 cores is a monotonic multiprocessor, since EV6 is strictly superior to EV5 in terms of hardware resources and virtually always performs better than EV5 for a given application given the same cycle time and latencies. The same goes for voltage/frequency scaling of identical cores.}, and the best heterogeneous designs are composed of specialized core architectures \cite{heterogeneous-arch}.
However, if pre-existing cores are not used, additional costs in design, verification and testing must be evaluated, to see if the benefits are worth the cost.

\todo{Improved. Does the list make sense now?.}
Several additional conclusions were reached in the study:
\begin{itemize}
\item The most efficient heterogeneous multiprocessors were not constructed from cores that make good general-purpose uniprocessor cores, or cores that would appear in a good homogeneous multicure architecture.
\item Each core should be individually tuned for a class of applications with common characteristics.
      This usually results in non-monotonic processors.
\item And performance advantages of heterogeneous multiprocessors, including non-monotonic also holds for completely homogeneous workloads (where the task had no differences).
    In those cases, the diversity across different workloads are exploited.
\end{itemize}
 %***NOTE: The list above makes no sense.
 % Torbjørn: Better now?

%\todo{Strongly considering deleting (commenting out) the following paragraph. Too much unnecessary details.}The workloads come from seven benchmarks from the SPEC suite, classified int o processor bound or bandwidth bound.
%The chosen benchmarks are carefully selected, and represents the entire SPEC suite.
%In additional, three other benchmarks from other suites are chosen as well.
%Every multiprocessor is evaluated on two classes of workloads, called \textit{all different} and \textit{all same}.
%\todo{Warning: Direct rip-off. Consider "quote"-form}The former consist of all possible 4-threaded combinations that can be constructed such that each of the 4 threads running at a different time.
%The latter consists of all combinations that can be constructed so that all the 4 threads running at a time are the same.

It turns out that the more constricted the available power or chip area is, the more gain custom heterogenity provides, as homogeneous multiprocessors could only perform any task at maximum capacity if the power budget was generous.
But as designs become more aggressive, more cores are expected on the die, and power budgets per core is likely to tighten more severly.
Heterogeneous designs dampen the effects of constrained power budgets. 
There were different best solutions for the varying power and area constraints, all with clear differences from the best homogeneous design.
This underlines the need to design from a clean slate, rather than modifying pre-existing core designs.
Such monotonous designs does not exploit heterogenity equally well, and is considered the upper bounds of the possible benefits from heterogeniety.
For example, an application may run better on an EV8 than EV6 because of larger caches, while other advanced features of EV8 remains underutulized and is thus wasted.
\cite{heterogeneous-arch}.
%Torbjørn: Better now?

%Any of the best heterogeneous designs tested for different power and area constraints were very different from the best homogeneous design, underlining the need to design heterogeneous processors from a clean slate, rather than modifying an existing core design.
%Monotonous designs, for instance, different generations of cores from same family, does not exploit heterogenity equally well.
%For instance, the benchmark \textit{mcf} from \cite{heterogeneous-ee} were mapped to EV6 or a single-threaded EV8-, in spite of these having low instruction level parallelism.
%The reason for this was that the larger caches of these cores causes the advanced capabilities of these larger cores to be underutilized. % %**NOTE: Makes no sense?

%Heterogeneous multicore processors designed from clean slate gives better performance.

%\subsection{Uncore}
%\todo{Client worklod description has been cut out. Reconsider if it should be included}
%\label{subsec:rw_uncore}
%Gupta \textit{et al}\cite{heterogeneous-uncore} ran experiments with a heterogeneous multiprocessor, with the goal of measuring the impact of "uncore" power on the energy efficiency of heterogeneous multicore processors.
%In this experiment, they focused on the use of client devices, where energy is a premium resource and the workload profiles are diverse.
%The uncore is a collection of components of a processor, not in the core, but essential for core performance.
%Some of these components are the last level cache (LLC), integrated memory controllers (IMC), on-chip interconnect (OCI), power control logic (PWR), etc.
%With growing cache size and integration of various SoC components on the CPU die, the uncore is becoming an increasingly imporant contributor \cite{heterogeneous-uncore}.

%If a core has varying levels of sleep states, where the "deeper the sleep", the less power used, then uncore will be as active as the most active core.
%If three cores are idle (or on low activity), and one core is on highest activity, then the uncore will be on the highest activity.
%If for a task, the choice is between a big fast core, or a more power efficient small core, and the small core is chosen, the ucore will stay active for longer time, impacting the possible energy saving.
%This can be seen illustrated in figure \ref{fig:Uncore1}.

%\begin{figure}[htb]
%    \centering
%    \includegraphics[width=1.0\textwidth]{Figures/Heterogeneous/Uncore1}
%    \caption{Effect of uncore power on the energy efficiency of heterogeneous cores, as seen in \cite{heterogeneous-uncore}.}
%    \label{fig:Uncore1}
%\end{figure}


%\todo{WARNING}\textbf{Skipped client workload description}

%The analysis considered two uncore conficurations: fixed and scalable.
%The first one used the same uncore subsystem for both big and small cores.
%The second modeled an uncore where certain components were turned off or powered down when moving to small cores.
%Examples are fewer memory channels, controllers, or smaller caches used.  

%\begin{figure}[htb]
%    \centering
%    \includegraphics[width=1.0\textwidth]{Figures/Heterogeneous/Uncore2}
%    \caption{Energy savings of using small cores, with core-only savings (C), and with SoC-wide savings (C+UC), with a fixed uncore, and with a scalable uncore, as seen in \cite{heterogeneous-uncore}.}
%    \label{fig:Uncore2}
%\end{figure}

%Figure \ref{fig:Uncore2} shows the energy saved when going from big to small core, with both fixed and scaled uncores.
%It is clear that without scaling of the uncores, energy saving is reduced, even wasted in some cases.
%Figure \ref{fig:Uncore3} shows the relative contribution of core and uncore energy consumption for all the applications during big core execution, on a fixed uncore configuration.

%Clearly it is important to take uncore power into account for scheduling operations, and design of scalable uncore design is motivated, to obtain better gains from heterogeneous multicores. 

%\begin{figure}[htb]
%    \centering
%    \includegraphics[width=1.0\textwidth]{Figures/Heterogeneous/Uncore3}
%    \caption{Core and uncore energy contricution for big cores and fixed uncores, as seen in \cite{heterogeneous-uncore}.}
%    \label{fig:Uncore3}
%\end{figure}

\section{The Single-ISA Heterogeneous MAny-core Computer}
\label{sec:shmac}

\todo{Did you cut out some content by deleting and not commenting?!}
The Single-ISA Heterogeneous MAny-core Computer is an architecture designed for investigating heterogeneous
systems at all abstraction levels, as illustrated in figure \ref{fig:shmacAbstractionLevels}.

The programming model is kept constant across SHMAC instances, while the underlying implementation can change,
allowing software to be written once and run on different SHMAC instances.

It implements a tile-based architecture with a mesh interconnect. All processor tiles implement the same
ARM ISA and the same memory model, in order to achieve a common programming model \cite{shmac-plan}.
A high-level illustration of SHMAC can be seen in figure \ref{fig:shmac}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/Heterogeneous/SHMACAbstractionLevels}
    \caption{Levels of abstraction in computing systems \cite{shmac-plan}.}
    \label{fig:shmacAbstractionLevels}
\end{figure}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.95\textwidth]{Figures/Heterogeneous/SHMAC}
    \caption{High-Level architecture of SHMAC \cite{shmac-plan}.}
    \label{fig:shmac}
\end{figure}

\subsection{SHMAC Architecture}

SHMAC is a tile-based architecture, with the processing elements laid out in a rectangular
grid with neighbour-to-neighbour connections, using a mesh interconnect with XY-routing.
%Several tile types are currently supported, not including the tile described in this report,
%with the most important ones being the processor tile, which contains a Turbo Amber CPU
%\footnote{See section \ref{sec:aht} for a short overview}, a scratchpad tile which functions as
%fast temporary storage, a DRAM tile and an I/O tile for communicating with a host system.

Several tile types are supported. The main ones are:

\begin{description}
    % Torbjørn: Joined Amber and Turbo Amber.
  \item[Processor Tile] Currently two versions are supported: the open source ARM Amber CPU, and an improved version called Turbo Amber.
  	The original Amber core provides support for the ARMv2a instruction set, while the Turbo Amber
	provides support for the ARMv4t instruction set, giving the core the ability to also execute
	instructions from the 16-bit Thumb instruction set. Several performance optimizations are
	also added to the core \cite{turboamber}. An overview of the processor tile can be seen in
	figure \ref{fig:shmac-cpu}.
  \item[Scratchpad Tile] --- A memory tile providing a small amount of memory for use by software.
  	This memory is provided by the on-chip block RAM resources of the FPGA \cite{shmac-plan}.
  \item[DDR Memory Tile] --- Memory controller tile that gives SHMAC access to off-chip DDR memory.
  \item[APB Interface (I/O tile)] --- This tile implements the Advanced Peripherals Bus (APB) slave
  	which gives the host processor on the Versatile Express board access to SHMACs memory space
	for use when programming the memories \cite{shmac-plan}.
  \item[Dummy Tile] --- Empty tile which only contains router functionality \cite{shmac-plan}. Can be used for instance
  	as a filler tile.
\end{description}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/Heterogeneous/SHMACCPU}
    \caption{SHMAC processor tile \cite{shmac-plan}.}
    \label{fig:shmac-cpu}
\end{figure}

\subsection{SHMAC Memory Map}
% *** NOTE: I seriously don't think this is relevant information, suggest removing this section.
% Torbjørn: Disagree, it affects our software programming, when using SHA-256 module, DMA module and Scratchpad memory. Everything that affects our project should be mentioned.

Figure \ref{fig:shmac-memory} shows how the memory is mapped in the ARM-based SHMAC. All
processor tiles share the same address space with only the tile register space being
private for each tile. This memory area contains information about the tile itself, such as its coordinates,
the CPU ID number and other useful data. In addition the tile register space contains the memory-mapped
peripherals for each tile, such as timers and the interrupt controller.
Any custom peripheral, such as the DMA and the hashing accelerator developed in the project is mapped into this address space.
The system registers are used for communication with the host system.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/Heterogeneous/SHMACMemory}
    \caption{Memory map of ARM-based SHMAC, as seen in \cite{shmac-plan}.}
    \label{fig:shmac-memory}
\end{figure}

\subsection{SHMAC Interconnection Network}

\todo{Lot of "uses" and "used". Could use language improvement}SHMAC uses a 2D mesh-based interconnection network to connect all the tiles, which is used
to transport data packets of 128~bits length.
Store-and-forward switching with on/off flow control is used, and the XY-algorithm is used for deciding the route of each data packet.
Each tile consists of a router with five ports, one for each of its neighbours and the local connection.
When multiple packets are inbound from different directions, a round-robin scheme is used to arbitrate between which
packet to route through, and packets from the local tile has equal priority to those arriving from the other directions.

In the current implementation, it takes 3 cycles for a data packet to transfer from one tile to the next. 
The architecture of the router can be seen in figure \ref{fig:shmac-router}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/Heterogeneous/SHMACRouter}
    \caption{Architecture of the SHMAC router \cite{shmac-plan}.}
    \label{fig:shmac-router}
\end{figure}

\subsection{Wishbone Bus}

\todo{Again, did you delete content instead of commenting out?! Now I'll have to dig it up again from somewhere on git.}
%Granted, the "purpose" part is probably not necessary, but I hoped it showed some insight on our knowledge of the system, maybe it could impress the cencor a little bit?

Wishbone is a bus architecture developed by OpenCores to create a common interface for use between IP cores, especially in
open source designs. It supports single-word transfers as well as burst transfers. A pipelined transfer mode is also provided
which allows multiple requests to be sent from a module without the module having to wait for an acknowledge.

Wishbone is used as internal bus on the CPU tiles of SHMAC. Burst or pipelined transfers are
not currently supported, neither internally on the tiles, or by the interconnect network, which can limit performance. \cite{Yaman}.

%WISHBONE is implemented as internal bus on the CPU tiles of SHMAC, but currently supports only single transfers with no bursts or pipelining.
%This means that in spite of using switched packet interconnection network on SHMAC, something that opens up for sending out several packets, a WB Master on a tile will only send out one request, before getting the acknowledged response back.
%\todo{Yaman said this. Find out if oral conversations with teachers/staff is allowed for bibliography.}\cite{Yaman}
%And prior to this project, only one WB master existed on each tile, owning the entire WISHBONE network in the tile.
%
% % ***NOTE: Actually, the mesh interconnect does not support burst transfers as far as I understood. It is hard not to support Wishbone burst transfers... and almost no one uses pipelined transfers in Wishbone.
%  Torbjørn: So, we just mention it. Easy-peasy
