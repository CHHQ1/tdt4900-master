\chapter{Background}
\label{cha:background}

Insert smart intro here:

"I'm SMAAAART!!!"

\section{Rise of Dark Silicon, and the four areas of solution}
\label{sec:dark-silicon}
As Taylor\cite{dark-silicon} points out, transistor density on a CMOS chip continue to double every two years, according to Moore's Law.
Native transistor speed also increases with a factor of 1,4x.
Energy efficiency on the other hands improves only with 1,4x, and under a constant power budget, it will cause a 2x shortfall in energy budget to power a chip at its native frequency.
The utulization of a chip's potential is thus falling expentionally by 2x per generation.
If the power limitation were to be based on the current generation, then designs would be 93,75\% dark in eight years.
This gives the rise to the term "Dark Silicon", the chip must either be underclocked, or parts of it turned off in order to keep within a set power budget.
This is essentially true for chips where \todo{Hmmm.... sounds cliche} traditional cooling no longer can be efficient enough to \todo{source needed}remove generated heat from a fully powered chip.

According to Dennard scaling, progress were measured by the improvement in transistor speed and count, while according to the new post-Dennard scaling the progress will now be measured by improvement in transistor energy efficiency.
While reducing delays have been the previous focus, the focus will now be to utilize as much joule from the transistors as possible.

The transistion from single-core to multicore processors in 2005 was a direct response from the industry to this problem, but adding multiple cores does not circumvent the problem on a longer run.
Multicore chips will not scale as transistors shrink, and the fraction of a chip that can be filled with cores running at full frequency is dropping exponentially with each processor generation. 
Large fractions of the chip will be left dark - either idle for a long time, or significantly underclocked.
Hence new designs are required, where new architectural techniques "spend" area to "buy" energy efficiency.

\subsection{The Utilization Wall}
\todo{Tror dette blir overflødig, men har lagt av plass sånn i tilfelle}
Considered optional for now.
If written, should contain the details of why there is the Utulization Wall that leads to dark silicon. 

\subsection{The four horsemen: Approaches to the dark silicon problem}
Taylor explains a taxonomy called "The four horsemen" \cite{dark-silicon}.
These are four proposed responses that are emerging as solutions as one transition beyond the transitional multicore stop-gap solution. \cite{dark-silicon2}
When looking back, these responses appeared to be unlikely candidates from the beginning, bringing with them unwelcome burdens in design, manufacturing and programming.
From an aestethic engineering point of view, none of them would appear ideal.
Henceforth the term "Horsemen".
But it can be seen from the success of complex multi-regime devices like MOSFETs that engineering as a field has an enormous tolerance for complexity if the end result is better.
From this result, Taylor argues that future chips will apply not just one of these alternatives, but all of them.

The four horsemen are called The Shrinking Horseman, The Dim Horseman, The Specialized Horseman and The Dues Ex Machina Horseman.

\subsubsection{The Shrinking Horseman}
Instead of having dark silicon on the chip, one may simply shrink the chip itself.
Taylor\cite{dark-silicon} views these "shrinking chips" as the most pessimistic outcome.
Although all chips may eventually shrink somewhat, the ones that shrink the most will be those where dark silicon cannot be applied fruitfully to improve the product.
\todo{Nært overflødig?}These chips will rapidly turn into low-margin businesses for which further generations of Moore’s law provide small benefit.
Futhermore, there are other effects: Exponentially smaller chips are not exponentially cheaper, since mask cost, design cost and I/O pad aread cannot be amortized. 
Competition will most likely favor chips that utulizes dark silicon to improve overall product, causing chips that are only shrinked to sell at low market price, causing loss to the company.
And last, but not least, exponential shrinking leads to exponential rise in power density, and chip temperature will thus follow suit.
Meeting the temperature limit will reduce the scaling below the nominal 1,4x expected energy efficienty. 

%There are also second-order effects assosiated with shrinking chips:

%\begin{itemize}
%    \item Exponentially smaller chips are not exponentially cheaper.
%    In addition to the silicon itself, cost include mask costs, design costs and I/O pad area.
%    These cannot be amortized, and the price will increase per mm$^2$, as the chip shrinks.
%    \item Shrinking the silicon can also shrink the chip selling price, but competition will likely force companies to utilize dark silicon if it can attain a benefit for the end product.
%    Companies who would rely on shrinking only, may loose the competition and sell chips at catastrophically decreased chip prices.
%    \item Exponential shrinking leads to exponential rise in power density, and chip temperature will thus follow suit.
%    Meeting the temperature limit will reduce the scaling below the nominal 1,4x expected energy efficienty. 
%\end{itemize}

\subsubsection{The Dim Horseman}
%\todo{This subchapter ended up long. Try to see if it can be reduced. Especially NTV}
According to Taylor\cite{dark-silicon}, as exponentially larger fractions of a chip’s transistors become dark transistors, silicon area becomes an exponentially cheaper resource relative to power and energy consumption.
Therefore new architectural techniques that spend area to buy energy efficiency is called for.
Instead of shrinking silicon, one may consider populating dark silicon area with logic that is used only part of the time, and interesting new design possibilities occurs.The term "dim silicon" refers to techniques that put large amounts of otherwise-dark silicon area to productive use by employing heavy underclocking or infrequent use to meet the power budget.
The architecture has to strategically managing the chip-wide transistor duty cycle to enforce the overall power constraint. 
Whereas early 90-nm designs such as Cell and Prescott were dimmed because actual power exceeded design-estimated power, more increasingly more elegant methods are converging, that make better trade-offs.Among the dim silicon techniques are dynamically varying the frequency with the number of cores being used, scaling up the amount of cache logic, employing near threshold voltage (NTV) processor designs, and redesigning the architecture to accommodate bursts that temporarily allow the power budget to be exceeded, such as Turbo Boost and computational sprinting.

%\todo{Consider shorting down or removing entire lists from each sub-chapter, to keep the four horsemen short and concise.}Amond dim silicon methods are: 

%\begin{itemize}
%    \item Turbo boost, where less core are active, the higher the frequency they may run at.
%    Energy gained from turning off cores is used to increase voltage and frequency of active cores.
%    This is known as Dynamic voltage and frequency scaling (DVFS).
%    \item Near-threshold voltage (NTV) Processors.
%    Transistors were at 2013 operated around 2,5x the threshold voltage, an energy-delay optimal point. 
%    This is at a point where reducing the voltage severely affects the frequency drop, which reduces the effective gain from downward-DVS.
%    Nevertheless, researchers have begun exploring this regime.
%    NTV logiv is a recent approach, where transistors in the near-threshold regime are operated slightly above the threshold voltage.
%    This provides a more palatable trade-off between energy and delay than subthreshold circuits, for which frequency drops exponentially with voltage decreases.
%    \todo{Give a proper mention of where the following numbers were taken from}
%    Although NTV per-processor performance drops faster than the corresponding savings in energy-per-instruction (5X energy improvement for an 8x performance cost), the perfor- mance loss can be offset by using 8x more processors in parallel if the workload allows it.
%    Then, an additional 5x processors could turn the energy efficiency gains into additional performance. 
%    \todo{Have to admit, I still don't understand how this is a gain. And this is almost the point of too much. Should remove the example, and find a different way to explain}So, with ideal parallelization, NTV could offer 5x the throughput improvement by absorbing 40x the area. 
%    But this would also require 40x more free parallelism in the workload relative to the parallelism consumed by an equivalent energylimited super-threshold many-core processor.%    \item Bigger caches.
%    Because only a subset of cache transistors (such as a wordline) is accessed each cycle, cache memories have low duty cycles and thus are inherently dark. 
%    Adding cache is therefore one way to simultaneously increase performance and lower power density per square millimeter.
%    But the less memory-bound a running application is, the less the benefit.
%    \item Computational sprinting and turbo boost.
%    One temporarily exceeds the nominal thermal budget but relies on thermal capacitance to buffer against temperature increases, and then ramp back to a comparatively dark state.
%    These are used within "race to finish" computations.

%\end{itemize}

\subsubsection{The Specialized Horseman}
This is the use of dark silicon to implement a host of specialized processors\cite{dark-silicon}.
They can be more energy efficient, or much faster than a general purpose processor.
Programs are executed where it is most efficient.
Unused cores are power- and clock gated in order to keep them from wasting energy.
\todo{Sort of ambigious on its own. Consider removal of sentence}
Specialized logic focuses on reducing the amount of capacitance that needs to be switched to perform a particular operation.

Specialization is already being realized today in forms of specialized accelerators that span diverse areas such as baseband processing, graphics, computer vision, and media coding.
These accelerators enable orders-of-magnitude improvements in energy efficiency and performance, especially for computations that are highly parallel.
It is expected to se a rise of systems with more coprocessors than general processors.
Tayler refers to them as coprocessor- dominated architectures, or CoDAs.

One of the expected challenges it the so-called "Tower of Babel" crisis, as the notion of general-purpose computation is fragmented, and the traditional clear lines of communication between programmers and software and the underlying hardware is eliminated.
For instance, CUDA for NVidia GPUs is not usable for similar architectures, such as AMD.
Overspecialization problems between accelerators that cause them to become inapplicable to closely related classes of computation has been observed.
In addition, adoption problems are also caused by the excessive costs of programming heterogeneous hardware (such as Sony Playstation 3 vs. Microsoft XBox), and there is always the risk that specialized hardware may become obsolete as standards are revised.

%\todo{The following sections are expanding challenges to the specialized horseman, beyond the basic. Consider carefully if this delves in too deply of our own project report, and delete it all if so.}
%The following challenges needs to be adressed:
%\begin{itemize}
%    \item To combat the "Tower of Babel" problem, it is required to define a new paradigm for how specialization is expressed and exploited in future processing systems. 
%    New scalable architectural schemas that employ pervasively specialized hardware to minimize energy and maximize performance are needed. At the same time they need to insulate the hardware designer and programmer from such systems’ underlying complexity.
%    \item  Amdahl’s law adds issues for specialization.
%    To save energy across the majority of the computation, broad-based specialization approaches that apply to both regular, parallel code and irregular code must be found.
%    It must also be ensured that communicating specialized processors doesn’t fritter away their energy savings on costly cross-chip communication or shared-memory accesses. 
%    \item Recent efforts. The UCSD GreenDroid processor. \todo{YEP!}Proposal: This is an example. Move and describe it in related works.
%\end{itemize}

\subsubsection{The Dues Ex Machina Horseman}
Taylor notes that this one is the most unpredictable\cite{dark-silicon}.
He uses the termology "Deus ex machina" from literature or theater, in which the protagonists seem increasingly doomed until the very last moment, when something completely unexpected comes out of nowhere to save the day.
In the case for dark silicon, one deus ex machina would be a breakthrough in semiconductor devices.
The required breakthrough would have to be very fundamental, making it possible to build transistors out of devices other than MOSFETs. 
There are physical limits to what can be done with \todo{I chose to not elaborate this one further}leakage from MOFSET transistors, and transistors made of other devices can combat this.
New transistors must also be able to compete with MOFSETs in performance.
Tunnel field-effect transistors (TFET) and nanoelectromechanical system (NEMS) switches are examples on inventions that hint to order-in-magnitude improvements in the leakage problem, but they still fall short in performance.

% TODO: Remove all occurrences of the word "horseman", it's a litterary reference we shouldn't use.

\section{Heterogeneous Architectures}
\label{sec:heterogeneous}


\subsection{The Single-ISA Heterogeneous MAny-core Computer}
\label{sec:shmac}
The Single-ISA Heterogeneous MAny-core Computer is a proposed infrastructure by NTNU for investigating heterogeneous systems at all abstraction levels, as illustrated in figure \cite{fig:shmacAbstractionLevels}.
The point is to create a flexible framework in which different heterogeneous processors can be created from a collection of processing elements and accelerators.
The programming model is kept constant across SHMAC-instances, while underlying implementation changes.
This way, software design space exploration is facilitated.
It is a tile-based architecture with a mesh interconnect.
All processor tiles implement the same ARM ISA and the same memory model, in order to achieve a common programming model.\cite{shmac-plan}.
An illustration of SHMAC can be seen in figure \cite{fig:shmac}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/Heterogeneous/SHMACAbstractionLevels}
    \caption{All levels of abstraction in computing systems, as seen in \cite{shmac-plan}.}
    \label{fig:shmacAbstracionLevels}
\end{figure}

\begin{figure}[htb]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/Heterogeneous/SHMAC}
    \caption{High-Level Architecture of ARM-based SHMAC, as seen in \cite{shmac-plan}.}
    \label{fig:shmac}
\end{figure}

\todo{PROBLEM: This paragraph is way too similar to original text. Must fix, one way or another. }NTNU argues that the SHMAC-approach gives the right tools to reach the research goles outlined in the plan in \cite{shmac-plan}.
For software research, SHMAC, makes it possible to explore substantially more diverse systems than the ones currently provided by the industry.
Furthermore, SHMAC-architectures realized in FPGAs will be significatly faster than using simulators.
It is expected that co-developing software and hardware will result in substantial cross-fertilization that gives insight into both hardware and software issues.
And finally, micro- and macro-architecture components can be combined with novel transistor technologies and ASIC realizations to reach research goals at the lower abstraction levels.

%Original source, kept for source comparison
%We believe that the SHMAC-approach gives us the right tools to reach the research goals outlined in this plan. For software research, SHMAC makes it possible to explore substantially more diverse systems than those cur- rently provided by the industry. At the same time, SHMAC-architectures realized in FPGAs will be significantly faster than simulator-based approaches. We expect that co-developing software and hardware result in substan- tial cross-fertilization that gives insights into both hardware and software issues. Finally, the SHMAC micro- and macro-architecture components can be combined with novel transistor technologies and ASIC realizations to reach research goals at the lower abstraction levels.

\subsubsection{SHMAC Architecture}

SHMAC is a tile-based architecture, with the processing elements laid out in a rectangular grid with neighbour-to-neighbour connection, using mesh interconnection.
Six tile types are supported, prior to this project\cite{shmac-plan}:

\todo{Entire following list pretty much rip-off, except Amber and Turbo Amber. Sources from elsewhere needed.}
\begin{description}
  \item[Amber Processor Tile] Processor tile that contains an ARM Amber processor, caches, peripherals and optional accelerators.
  It can be seen in figure \ref{fig:shmac-cpu}.
  \todo{This and next 2 sentences rip-off.}Currently, the peripherals consists of interrupt controller, timers and tile registers.
  The tile registers are per-tile memory that store local information like the processor ID, the coordinates of the tile, etc. 
  The caches, router and peripherals are connected to a Wishbone bus.
  
  \todo{IMPORTANT! Affected our results!}Caches reduce the average memory latency, but adding caches also add the possibility for coherence problems.
  This is currently solved by placing shared data in uncachable memory regions, through software. 
  
  The processor tile can be equipped with optional accelerators, designed to execute a specified computation in a very effective way.
  
  
  \item[Turbo Amber Processor Tile] \todo{Suggest source needed from Redmine or the internet}Same as previous, but has an ARM Turbo Amber processor with \todo{Needs to be confirmed} higher performance.
  \item[Scratchpad Tile] Includes memory and a router, but no processing element. 
  It is is an on-chip memory where the contents are managed by software (e.g. programmer, compiler, system software, etc.).
  Each scratchpad tile is given a 128 MB address space.
  The number of scratchpad tiles depends on a number of factors like the amount of block RAM available on the chosen FPGA, the desired access latency, the amount of block RAM used for caches in the processor tiles and the possibility of access contention\cite{shmac-plan}.
  The memory layout for SHMAC is seen in figure \ref{fig:shmac_memory}.
  \item[Main Memory Tile] Memory controller tile that gives SHMAC access to off-chip memory.
  \item[APB Interface] This tile implements the Advanced Peripherals Bus (APB) slave which gives the host processor on the Versatile boards access to SHMACs memory space. 
  In addition, the APB contains system registers which are used for managing communication with the host system.
  \item[Dummy Tile] Contains only a router and is used to fill remaining tiles when there is not enough resources available in the target FPGA to fill all tiles with tiles that implement functionality.
\end{description}


\begin{figure}[htb]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/Heterogeneous/SHMAC}
    \caption{SHMAC Processor tile, as seen in \cite{shmac-plan}.}
    \label{fig:shmac-cpu}
\end{figure}


%\subsubsection{SHMAC Memory Map}
%
%Figure \ref{fig:shmac-memory} shows how the memory is mapped in ARM-based SHMAC.
%
%The Exception table contains 8 instructions, one for each exception type described on \todo{Not mentioned so far in this document. Consider inclusion}Redmine.
%\todo{Rip-off}The high end of the memory space is reserved for system registers and tile registers. 
%The tile registers are private for each tile and contain information like the tiles coordinates and other useful data while the system registers are used for communication with the host system.
%
%The scratchpad tiles have been given a 128 MB address space. The SHMAC architecture supports 1 to k scratchpad tiles where k can be chosen independently of the number of tiles in the SHMAC instance. The number of scratchpad tiles depends on a number of factors like the amount of block RAM available on the chosen FPGA, the desired access latency, the amount of block RAM used for caches in the processor tiles and the possibility of access contention.

\begin{figure}[htb]
    \centering
    \includegraphics[width=1.0\textwidth]{Figures/Heterogeneous/SHMACMemory}
    \caption{Memory map of ARM-based SHMAC, as seen in \cite{shmac-plan}.}
    \label{fig:shmac-memory}
\end{figure}


%\subsubsection{Work packages}


\section{The Bitcoin Currency}
\label{sec:bitcoins}

Bitcoin is a new currency, based on cryptographic stuffz\todo{Eloquence}.

At the core of the bitcoin system is the block chain, a distributed linked-list consisting of blocks
which contains the transactions that have been executed since the previous block was generated.

A block is only valid if the arithmetic value of the double SHA-256 hash of its header is below
a certain target value. The target value is decided by the network and is set to such a value that
on average six new blocks are generated per hour. \cite{bitcoin}

\subsection{Mining Bitcoins}
\label{sec:bitcoin-mining}

The process of creating a new block for the bitcoin blockchain is often referred to as \textit{mining}.

The process begins with the creation of a transaction that transfers the reward for generating the block
into the account of the miner. This transaction is called the coinbase transaction. All transactions
transmitted to the bitcoin network since the last block was generated are gathered and a merkle tree
is constructed by combining the hashes of these transactions\todo{Explain merkle trees}.

The root of the merkle tree is inserted into the header for the new block together with the hash of the
previous block and various other fields specified by the standard. If the hash is below a target value,
determined by the current network difficulty, the block is successfully mined and transmitted to the
network.

\subsection{The SHA-256 Hashing Algorithm}
\label{sec:hashing-algo}

The SHA-256 algorithm is a member of a set of algorithms referred to as the SHA-2 standard.
These are described in \cite{fips180-4} and consists of algorithms for producing hashes with lengths of 224, 256, 384 and 512 bits.
The algorithms use simple operations, limited to shifts, rotates, xor, and unsigned additions,
common single-cycle operations for general purpose CPUs, in addition to a lookup-table of constants.
This allows for high-speed implementations in both software and hardware. The different SHA-2 algorithms
differ in how and with what parameters the various operations are invoked.

SHA-256 is the algorithm used in cryptocoin mining. It operates on blocks of 512~bits and keeps
a 256~bits long intermediate hash value as state. Bitcoin uses a double pass SHA-256 hash, which
first calculates the hash of a block of the data to be hashed and then hashes the hash of the first
pass.

Before the first block is processed, the initial hash value is set to a predefined
value. The entire message that is to be hashed is then padded by adding a 1~bit to
the end of the message and then appending zeroes until the length of the final block
%is 448̃~bits. Then the length of the entire message, without padding, is added as a
is 448-bits. Then the length of the entire message, without padding, is added as a
64-bit big-endian integer to the end of the block.

Then, each input block is split into a 64 times 32-bit long expanded message block, where
each 32-bit word $W_j$ is defined according to the formula

\[ W_j = \left\{
	\begin{array}{l l}
		M_j & \quad j \in \left[0, 15\right]\\
		\sigma_1(W_{j - 2}) + W_{j - 7} + \sigma_0(W-{j - 15}) + W_{j - 15} & \quad j \in \left[16, 63\right]
	\end{array}
\right.\]

\noindent where $M_j$ is the $j$th word of the input message block and the functions
$\sigma_0$ and $\sigma_1$ are defined as

\[\sigma_0 = R^7(x) \oplus R^{18}(x) \oplus S^3(x)\]
\[\sigma_1 = R^{17}(x) \oplus R^{19}(x) \oplus S^{10}(x)\]

\noindent where the operator $R^n$ means right rotation by $n$ bits and $S^n$ means right shift by $n$
bits \footnote{Curiously, \cite{sha-spec} defines the operator $R$ as shift and $S$ as rotate.
We use the more intuitive definitions.}.

\subsubsection{The Compression Function}
\label{sec:sha-compr}
The compression function is the core of the SHA-256 algorithm. It uses a look-up table
of 64 constants, $K_j$, and the following functions when calculating the new intermediate
hash values:

\[Ch(x,y,z) = (x \wedge y) \oplus (\neg x \wedge z)\]
\[Maj(x, y, z) = (x \wedge y) \oplus (x \wedge z) \oplus (y \wedge z)\]
\[\Sigma_0(x) = R^2(x) \oplus R^{13}(x) \oplus R^{22}(x)\]
\[\Sigma_1(x) = R^6(x) \oplus R^{11}(x) \oplus R^{25}(x)\]

Before starting the iterations with the compression function, the intermediate
hash values from the previous message block are assigned to the variables $a$--$h$.

At the beginning of each iteration of the compression function, two temporary
values are calculated:

\[T_1 = h + \Sigma_1(e) + Ch(e, f, g) + K_j + W_j\]
\[T_2 = \Sigma_0(a) + Maj(a, b, c)\]

The new hash values are then assigned as follows:

\[\begin{array}{l}
	h \leftarrow g \\
	g \leftarrow f \\
	f \leftarrow e \\
	e \leftarrow d + T_1\\
	d \leftarrow c \\
	c \leftarrow b \\
	b \leftarrow a \\
	a \leftarrow T_1 + T_2 \\
\end{array}\]

The compression function is run 64 times, once for each word in the extended message block,
$W_j$. Afterwards, the intermediate hash for the message is updated by adding the
variables $a$--$h$ to the corresponding values of the intermediate hash values from
the previous message block.

When the final input block has been processed, the final hash is composed by
concatenating the intermediate hash values \cite{sha-spec}.
